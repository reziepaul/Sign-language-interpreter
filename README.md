# Sign-language-interpreter
A real-time sign language interpreter using MediaPipe, OpenCV, and a custom-trained CNN model in TensorFlow/Keras that recognizes hand gestures via webcam and translates them into speech or text for inclusive communication

This project uses **OpenCV** for real-time webcam access and image processing, **MediaPipe** for accurate hand landmark detection, **CVZone** for simplified hand tracking, and **TensorFlow/Keras** to build and train a custom **Convolutional Neural Network (CNN)**. The model is trained on a self-collected dataset of hand gestures, where each gesture is cropped, resized onto a white square background to maintain aspect ratio, and labeled accordingly. These processed images are then used to train the CNN to classify gestures like "Hello", "Thank You", "Yes", "No", "I Love You", and "Sorry". During testing, the live webcam feed is analyzed in real time: MediaPipe detects the hand, the region of interest is cropped and resized, and the trained model predicts the gesture. The result is displayed on the screen and optionally spoken aloud using `pyttsx3`, creating an accessible and interactive user experience.
